---
---
@article{phung2023wavelet,
  abbr = {CVPR},
  title={Wavelet Diffusion Models are fast and scalable Image Generators},
  author={Phung, Hao and Dao, Quan and Tran, Anh},
  journal={Conference on Computer Vision and Pattern Recognition},
  year={2023},
  selected={true},
  published={true},
  pdf = {https://arxiv.org/abs/2211.16152},
  code = {https://github.com/VinAIResearch/WaveDiff.git},
  abstract = {Diffusion models are rising as a powerful solution for high-fidelity image generation, which exceeds GANs in quality in many circumstances. However, their slow training and inference speed is a huge bottleneck, blocking them from being used in real-time applications. A recent DiffusionGAN method significantly decreases the models' running time by reducing the number of sampling steps from thousands to several, but their speeds still largely lag behind the GAN counterparts. This paper aims to reduce the speed gap by proposing a novel wavelet-based diffusion structure. We extract low-and-high frequency components from both image and feature levels via wavelet decomposition and adaptively handle these components for faster processing while maintaining good generation quality. Furthermore, we propose to use a reconstruction term, which effectively boosts the model training convergence. Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets prove our solution is a stepping-stone to offering real-time and high-fidelity diffusion models. Our code and pre-trained checkpoints will be available at https://github.com/VinAIResearch/WaveDiff.git.},
  bibtex_show={true},
  preview={wavediff_teaser.png}
}

@INPROCEEDINGS{9335891,
  abbr = {NICS},
  author={Vo*, Huy Quoc and Phung*, Tien-Hao and Ly, Ngoc Quoc},
  booktitle={2020 7th NAFOSTED Conference on Information and Computer Science (NICS)}, 
  title={VQASTO: Visual Question Answering System for Action Surveillance based on Task Ontology}, 
  year={2020},
  volume={},
  number={},
  pages={273-279},
  doi={10.1109/NICS51282.2020.9335891},
  selected={false},
  published={true},
  pdf = {https://ieeexplore.ieee.org/abstract/document/9335891},
  abstract = {Question answering (QA) is a popular research topic for its applications in reality. In advance, there are Visual Question Answering (VQA) researches that aim to combine visual and textual information for question answering. Their drawback is the dependence of learning models, which impedes human intervention and interpretation. To the best of our knowledge, most of them concentrate on the general problem or some specific contexts but no one puts the QA system under action surveillance context. In this paper, we propose a QA system based on Task Ontology which is mainly responsible for mapping from a question sentence to corresponding tasks carried out to reach the appropriate answer. The advantages of task ontology are the adoption of human knowledge to solve a specific problem and the reusability. The performance of the system thus heavily depends on subtasks/models. In our scope, we focus on two main subtasks: Pose estimation/tracking and Skeleton-based action recognition. Besides, we give some enhancements to improve the time efficiency of Pose estimation/tracking and propose a new spatial-temporal feature based on drawing skeleton sequence to image for skeleton-based action recognition of videos in the wild. This method, to some extent, can overcome the challenge of bad-shape pose/skeleton produced by Pose estimation on real-world videos. The hard part of Action recognition in VQASTO is that it has to get input from Pose estimation and Pose tracking which is markedly different from having available good skeletons and merely do recognition.},
  dimensions={false},
  bibtex_show={true}
}
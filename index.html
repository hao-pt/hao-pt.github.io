<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Hao Phung</title> <meta name="author" content="Hao Phung"> <meta name="description" content=""> <meta name="keywords" content="artificial-intelligence, computer-vision, deep-learning, generative-models"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://hao-pt.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%74%69%65%6E%68%61%6F%70%68%75%6E%67@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=KxSdvGoAAAAJ&amp;hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2192207316" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/hao-pt" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/tienhaophung" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/tienhaophung" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Hao</span> Phung </h1> <p class="desc">CS PhD student @ Cornell University, Cornell Tech, NYC</p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address" style="font-size: 14px;"> <center> <p>Manhattan, NYC</p> </center> </div> </div> <div class="clearfix"> <p>Greetings! I am a PhD student in Computer Science at Cornell Tech, NYC, advised by <a href="https://www.hadarelor.com/" rel="external nofollow noopener" target="_blank">Hadar Averbuch-Elor</a>. Previously, I was an AI Research Resident at <a href="https://www.vinai.io/" rel="external nofollow noopener" target="_blank">VinAI</a>, where I was fortunate to work under the mentorship of Dr. <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&amp;hl=en/" rel="external nofollow noopener" target="_blank">Anh Tran</a>. Before that, I received my Bachelor of Computer Science from <a href="https://www.hcmus.edu.vn/" rel="external nofollow noopener" target="_blank">Ho Chi Minh City University of Science</a> (HCMUS) in Vietnam.</p> <p>My primary research interests center on generative models in vision and language domains, with an increasing focus on multimodal systems.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep 18, 2025</th> <td> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <a href="">E2D2</a>, a paper I co-authored, has been accepted to NeurIPS 2025. </td> </tr> <tr> <th scope="row">Jun 2, 2025</th> <td> <img class="emoji" title=":sunny:" alt=":sunny:" src="https://github.githubassets.com/images/icons/emoji/unicode/2600.png" height="20" width="20"> I have joined Apple AIML as a research intern, working with <a href="https://hxyou.github.io/" rel="external nofollow noopener" target="_blank">Haoxuan You</a>. </td> </tr> <tr> <th scope="row">Jan 22, 2025</th> <td> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <a href="https://arxiv.org/abs/2412.10193" rel="external nofollow noopener" target="_blank">Discrete Diffusion Guidance</a> has been accepted to ICLR 2025. </td> </tr> <tr> <th scope="row">Dec 9, 2024</th> <td> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <a href="https://arxiv.org/abs/2412.16906" rel="external nofollow noopener" target="_blank">Self-Corrected Flow Distillation</a> has been accepted to AAAI 2025. </td> </tr> <tr> <th scope="row">Sep 25, 2024</th> <td> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <a href="https://hao-pt.github.io/dimsum/">DiMSUM</a> has been accepted to NeurIPS 2024. </td> </tr> <tr> <th scope="row">Aug 26, 2024</th> <td> <img class="emoji" title=":baby_chick:" alt=":baby_chick:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f424.png" height="20" width="20"> I started my PhD studies at Cornell University. </td> </tr> <tr> <th scope="row">Jul 14, 2023</th> <td> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <a href="https://arxiv.org/abs/2303.15433" rel="external nofollow noopener" target="_blank">Anti-DreamBooth</a> has been accepted to ICCV 2023. </td> </tr> <tr> <th scope="row">Feb 28, 2023</th> <td> <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <a href="https://arxiv.org/abs/2211.16152" rel="external nofollow noopener" target="_blank">WaveDiff</a> has been accepted to CVPR 2023. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/discrete_guidance.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="discrete_guidance.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schiff2024discreteguidance" class="col-sm-8"> <div class="title">Simple Guidance Mechanisms for Discrete Diffusion Models</div> <div class="author"> Yair Schiff*, Subham Sekhar Sahoo*, <em>Hao Phung*</em>, Guanghan Wang*, Sam Boshar, Hugo Dalla-torre, Bernardo P Almeida, Alexander Rush, Thomas Pierrot, and Volodymyr Kuleshov</div> <div class="periodical"> <em>In ICLR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-eye"></i>  Abs</a> <a href="http://arxiv.org/abs/2412.10193" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i>  arXiv</a> <a class="bibtex btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-quote-right"></i>  Bib</a> <a href="https://github.com/kuleshov-group/discrete-diffusion-guidance.git" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i>  Code</a> <a href="https://discrete-diffusion-guidance.github.io/" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-globe"></i>  Page</a> </div> <div class="abstract hidden"> <p>Diffusion models for continuous data gained widespread adoption owing to their high quality generation and control mechanisms. However, controllable diffusion on discrete data faces challenges given that continuous guidance methods do not directly apply to discrete diffusion. Here, we provide a straightforward derivation of classifier-free and classifier-based guidance for discrete diffusion, as well as a new class of diffusion models that leverage uniform noise and that are more guidable because they can continuously edit their outputs. We improve the quality of these models with a novel continuous-time variational lower bound that yields state-of-the-art performance, especially in settings involving guidance or fast generation. Empirically, we demonstrate that our guidance mechanisms combined with uniform noise diffusion improve controllable generation relative to autoregressive and diffusion baselines on several discrete data domains, including genomic sequences, small molecule design, and discretized image generation. Code is available at https://github.com/kuleshov-group/discrete-diffusion-guidance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">schiff2024discreteguidance</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Simple Guidance Mechanisms for Discrete Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schiff*, Yair and Sahoo*, Subham Sekhar and Phung*, Hao and Wang*, Guanghan and Boshar, Sam and Dalla-torre, Hugo and de Almeida, Bernardo P and Rush, Alexander and Pierrot, Thomas and Kuleshov, Volodymyr}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/scflow.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="scflow.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dao2024scflow" class="col-sm-8"> <div class="title">Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Image Generation</div> <div class="author"> <a href="https://quandao10.github.io/" rel="external nofollow noopener" target="_blank">Quan Dao*</a>, <em>Hao Phung*</em>, <a href="https://trung-dt.com/" rel="external nofollow noopener" target="_blank">Trung Dao</a>, <a href="https://people.cs.rutgers.edu/~dnm/" rel="external nofollow noopener" target="_blank">Dimitris N. Metaxas</a>, and <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ" rel="external nofollow noopener" target="_blank">Anh Tran</a> </div> <div class="periodical"> <em>In AAAI</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-eye"></i>  Abs</a> <a href="http://arxiv.org/abs/2412.16906" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i>  arXiv</a> <a class="bibtex btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-quote-right"></i>  Bib</a> <a href="https://github.com/VinAIResearch/SCFlow" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i>  Code</a> </div> <div class="abstract hidden"> <p>Flow matching has emerged as a promising framework for training generative models, demonstrating impressive empirical performance while offering relative ease of training compared to diffusion-based models. However, this method still requires numerous function evaluations in the sampling process. To address these limitations, we introduce a self-corrected flow distillation method that effectively integrates consistency models and adversarial training within the flow-matching framework. This work is a pioneer in achieving consistent generation quality in both few-step and one-step sampling. Our extensive experiments validate the effectiveness of our method, yielding superior results both quantitatively and qualitatively on CelebA-HQ and zero-shot benchmarks on the COCO dataset. Our implementation is released at https://github.com/VinAIResearch/SCFlow.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dao2024scflow</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Image Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dao*, Quan and Phung*, Hao and Dao, Trung and Metaxas, Dimitris N. and Tran, Anh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The 39th Annual AAAI Conference on Artificial Intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/dimsum.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="dimsum.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="phung2024dimsum" class="col-sm-8"> <div class="title">DiMSUM: Diffusion Mamba - A Scalable and Unified Spatial-Frequency Method for Image Generation</div> <div class="author"> <em>Hao Phung*</em>, <a href="https://quandao10.github.io/" rel="external nofollow noopener" target="_blank">Quan Dao*</a>, <a href="https://trung-dt.com/" rel="external nofollow noopener" target="_blank">Trung Dao</a>, <a href="https://viethoang1512.github.io/" rel="external nofollow noopener" target="_blank">Hoang Phan</a>, <a href="https://people.cs.rutgers.edu/~dnm/" rel="external nofollow noopener" target="_blank">Dimitris N. Metaxas</a>, and <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ" rel="external nofollow noopener" target="_blank">Anh Tran</a> </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-eye"></i>  Abs</a> <a href="http://arxiv.org/abs/2411.04168" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i>  arXiv</a> <a class="bibtex btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-quote-right"></i>  Bib</a> <a href="https://github.com/VinAIResearch/DiMSUM.git" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i>  Code</a> <a href="https://vinairesearch.github.io/DiMSUM/" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-globe"></i>  Page</a> </div> <div class="abstract hidden"> <p>We introduce a novel state-space architecture for diffusion models, effectively harnessing spatial and frequency information to enhance the inductive bias towards local features in input images for image generation tasks. While state-space networks, including Mamba, a revolutionary advancement in recurrent neural networks, typically scan input sequences from left to right, they face difficulties in designing effective scanning strategies, especially in the processing of image data. Our method demonstrates that integrating wavelet transformation into Mamba enhances the local structure awareness of visual inputs and better captures long-range relations of frequencies by disentangling them into wavelet subbands, representing both low- and high-frequency components. These wavelet-based outputs are then processed and seamlessly fused with the original Mamba outputs through a cross-attention fusion layer, combining both spatial and frequency information to optimize the order awareness of state-space models which is essential for the details and overall quality of image generation. Besides, we introduce a globally-shared transformer to supercharge the performance of Mamba, harnessing its exceptional power to capture global relationships. Through extensive experiments on standard benchmarks, our method demonstrates superior results compared to DiT and DIFFUSSM, achieving faster training convergence and delivering high-quality outputs. The codes and pretrained models are released at https://github.com/VinAIResearch/DiMSUM.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">phung2024dimsum</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DiMSUM: Diffusion Mamba - A Scalable and Unified Spatial-Frequency Method for Image Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Phung*, Hao and Dao*, Quan and Dao, Trung and Phan, Hoang and Metaxas, Dimitris N. and Tran, Anh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/antidb_trump.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="antidb_trump.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="le_etal2023antidreambooth" class="col-sm-8"> <div class="title">Anti-DreamBooth: Protecting users from personalized text-to-image synthesis</div> <div class="author"> <a href="https://github.com/Luvata" rel="external nofollow noopener" target="_blank">Thanh Van Le*</a>, <em>Hao Phung*</em>, <a href="https://scholar.google.com/citations?user=6xS-7kMAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Thuan Hoang Nguyen*</a>, <a href="https://quandao10.github.io/" rel="external nofollow noopener" target="_blank">Quan Dao*</a>, Ngoc Tran, and <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ" rel="external nofollow noopener" target="_blank">Anh Tran</a> </div> <div class="periodical"> <em>In ICCV</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-eye"></i>  Abs</a> <a href="http://arxiv.org/abs/2303.15433" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i>  arXiv</a> <a class="bibtex btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-quote-right"></i>  Bib</a> <a href="https://github.com/VinAIResearch/Anti-DreamBooth" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i>  Code</a> <a href="https://anti-dreambooth.github.io/" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-globe"></i>  Page</a> </div> <div class="abstract hidden"> <p>Text-to-image diffusion models are nothing but a revolution, allowing anyone, even without design skills, to create realistic images from simple text inputs. With powerful personalization tools like DreamBooth, they can generate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual victim, posing a severe negative social impact. In this paper, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each user’s image before publishing in order to disrupt the generation quality of any DreamBooth model trained on these perturbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of DreamBooth and Diffusion-based text-to-image models, our methods effectively defend users from the malicious use of those models. Their effectiveness withstands even adverse conditions, such as model or prompt/term mismatching between training and testing. Our code will be available at https://github.com/VinAIResearch/Anti-DreamBooth.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">le_etal2023antidreambooth</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Anti-DreamBooth: Protecting users from personalized text-to-image synthesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Le*, Thanh Van and Phung*, Hao and Nguyen*, Thuan Hoang and Dao*, Quan and Tran, Ngoc and Tran, Anh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/wavediff_denoising.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="wavediff_denoising.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="phung2023wavelet" class="col-sm-8"> <div class="title">Wavelet Diffusion Models are fast and scalable Image Generators</div> <div class="author"> <em>Hao Phung*</em>, <a href="https://quandao10.github.io/" rel="external nofollow noopener" target="_blank">Quan Dao*</a>, and <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ" rel="external nofollow noopener" target="_blank">Anh Tran</a> </div> <div class="periodical"> <em>In CVPR</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-eye"></i>  Abs</a> <a href="http://arxiv.org/abs/2211.16152" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i>  arXiv</a> <a class="bibtex btn btn-sm z-depth-0 border-0 rounded-pill" role="button"><i class="fas fa-quote-right"></i>  Bib</a> <a href="https://github.com/VinAIResearch/WaveDiff.git" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i>  Code</a> <a href="https://youtu.be/KaIMMamhKsU" class="btn btn-sm z-depth-0 border-0 rounded-pill" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-video"></i>  Video</a> </div> <div class="abstract hidden"> <p>Diffusion models are rising as a powerful solution for high-fidelity image generation, which exceeds GANs in quality in many circumstances. However, their slow training and inference speed is a huge bottleneck, blocking them from being used in real-time applications. A recent DiffusionGAN method significantly decreases the models’ running time by reducing the number of sampling steps from thousands to several, but their speeds still largely lag behind the GAN counterparts. This paper aims to reduce the speed gap by proposing a novel wavelet-based diffusion scheme. We extract low-and-high frequency components from both image and feature levels via wavelet decomposition and adaptively handle these components for faster processing while maintaining good generation quality. Furthermore, we propose to use a reconstruction term, which effectively boosts the model training convergence. Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets prove our solution is a stepping-stone to offering real-time and high-fidelity diffusion models. Our code and pre-trained checkpoints will be available at https://github.com/VinAIResearch/WaveDiff.git.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">phung2023wavelet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Wavelet Diffusion Models are fast and scalable Image Generators}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Phung*, Hao and Dao*, Quan and Tran, Anh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%74%69%65%6E%68%61%6F%70%68%75%6E%67@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=KxSdvGoAAAAJ&amp;hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2192207316" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/hao-pt" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/tienhaophung" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/tienhaophung" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> Contact: hao (at) cs (dot) cornell (dot) edu | tienhaophung (at) gmail (dot) com </div> </div> <center> <script src="https://kit.fontawesome.com/0c7c27ff53.js" crossorigin="anonymous"></script> <a href="#" onclick="toggle_visibility('notice');toggle_chevron('toggle');" style="text-align:center;font-size:80%;color:#989898"> <i id="toggle" class="fa fa-chevron-circle-right"></i> hao-pt.github.io's clustrmaps 🌎 </a> <div id="notice" style="display:none; color:#dedede; font-size:.5em;"> <p> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=300&amp;t=tt&amp;d=SMzmCrtLul4PeBgV-hxmBaV2BV1GQQyxAWrtBobFAe8&amp;co=2d78ad&amp;cmo=e20808&amp;cmn=57e014&amp;ct=ffffff"></script> </p> </div> </center> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Hao Phung. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 20, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">function toggle_visibility(l){var e=document.getElementById(l);"block"==e.style.display?e.style.display="none":e.style.display="block"}</script> <script type="text/javascript">function toggle_chevron(c){const e="fa fa-chevron-circle-right",o="fa fa-chevron-circle-down",t=document.getElementById(c);toggle.onclick=(()=>{t.classList.toggle(e),t.classList.toggle(o)})}</script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>